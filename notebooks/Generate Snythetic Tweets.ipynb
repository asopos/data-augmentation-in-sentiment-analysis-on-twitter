{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INPUT_FILE = \"test-2\"\n",
    "\n",
    "data =pd.read_csv(\n",
    "    filepath_or_buffer=f\"../data/{INPUT_FILE}.csv\",\n",
    "    sep='\\t',\n",
    "    encoding='utf8',\n",
    "    names=[\"ID\", \"Label\", \"Tweet\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "DATA_MULTIPLIER = 3\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def token_pipeline(tweet):\n",
    "    # Lowercase the tweet\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove user mentions\n",
    "    tweet = re.sub(r'\\@\\w+', '', tweet)\n",
    "\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'\\#\\w+', '', tweet)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    tweet = re.sub(r'\\W', ' ', tweet)\n",
    "\n",
    "    # Remove digits and numbers\n",
    "    tweet = re.sub(r'\\d', '', tweet)\n",
    "\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data[\"Tweet_Token\"] = data[\"Tweet\"].apply(token_pipeline)\n",
    "\n",
    "data = pd.DataFrame(np.repeat(data.values, DATA_MULTIPLIER, axis=0), columns=data.columns)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create synthetic data with Synonyms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SYNONYM_PERCENTAGE = 0.2\n",
    "synonym_col_name = f\"Synonyms_Synthetic {SYNONYM_PERCENTAGE}\"\n",
    "def replace_words_with_synonyms(tweet_tokens, percentage=0.2):\n",
    "    tmp_tokens = tweet_tokens.copy()\n",
    "\n",
    "    num_to_replace = int(len(tmp_tokens) * percentage)\n",
    "\n",
    "    for i in range(num_to_replace):\n",
    "        rand_index = random.randint(0, len(tmp_tokens) - 1)\n",
    "\n",
    "        word = tmp_tokens[rand_index]\n",
    "\n",
    "        synsets = wordnet.synsets(word)\n",
    "        synonyms = set()\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "\n",
    "        if len(synonyms) > 0:\n",
    "            tmp_tokens[rand_index] = random.choice(list(synonyms))\n",
    "\n",
    "    return ' '.join(tmp_tokens)\n",
    "\n",
    "data[synonym_col_name] = data[\"Tweet_Token\"].apply(lambda tweet: replace_words_with_synonyms(tweet, SYNONYM_PERCENTAGE))\n",
    "\n",
    "data[[\"Tweet\", synonym_col_name, \"Tweet_Token\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create synthetic data with Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import fasttext, KeyedVectors\n",
    "\n",
    "\n",
    "fasttext_model = fasttext.load_facebook_model(path=\"H:\\\\wiki.simple.bin\").wv\n",
    "word2vec_model = KeyedVectors.load(\"H:\\\\word2vec-google-news-300\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T17:05:10.433693Z",
     "end_time": "2023-04-18T17:05:53.158174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "EMBEDDING_PERCENTAGE = 0.5\n",
    "fasttext_col_name = f\"fasttext_Synthetic {EMBEDDING_PERCENTAGE}\"\n",
    "word2vec_col_name = f\"word2vec_Synthetic {EMBEDDING_PERCENTAGE}\"\n",
    "def create_synthetic_tweet_word_embeddings(tokens, model, percentage=0.2):\n",
    "    tmp_tokens = tokens.copy()\n",
    "    num_words_to_replace = int(len(tmp_tokens) * percentage)\n",
    "    words_to_replace = random.sample(range(len(tmp_tokens)), num_words_to_replace)\n",
    "\n",
    "    for idx in words_to_replace:\n",
    "        word = tmp_tokens[idx]\n",
    "        try:\n",
    "            similar_words = model.most_similar(word, topn=3)\n",
    "            similar_words = [w for w, _ in similar_words if w.lower() != word.lower()]\n",
    "\n",
    "            if similar_words:\n",
    "                new_word = np.random.choice(similar_words)\n",
    "                tmp_tokens[idx] = new_word\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return \" \".join(tmp_tokens)\n",
    "\n",
    "#data[fasttext_col_name] = data[\"Tweet_Token\"].apply(lambda tweet: create_synthetic_tweet_word_embeddings(tweet, fasttext_model, EMBEDDING_PERCENTAGE))\n",
    "data[word2vec_col_name] = data[\"Tweet_Token\"].apply(lambda tweet: create_synthetic_tweet_word_embeddings(tweet, word2vec_model, EMBEDDING_PERCENTAGE))\n",
    "data[[\"Tweet_Token\", fasttext_col_name, word2vec_col_name]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T17:09:00.841843Z",
     "end_time": "2023-04-18T17:09:59.943702Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create synthetic data with GPT2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "SEED_PERCENTAGE = 0.5\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model_col_name = f\"GPT2_Synthetic {SEED_PERCENTAGE}\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "def generate_sentence(tweet_token, seed_percent=0.5):\n",
    "    length_of_seed_tokens = int(len(tweet_token) * seed_percent)\n",
    "    seed = \" \".join(tweet_token[0:length_of_seed_tokens])\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    input_text = tokenizer.encode(seed, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output = model.generate(input_text, max_length=len(tweet_token)*3, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_text = generated_text.replace(\"\\n\", \"\")\n",
    "    return generated_text\n",
    "\n",
    "data[model_col_name] = data[\"Tweet_Token\"].apply(lambda tweet_tokens: generate_sentence(tweet_tokens, SEED_PERCENTAGE))\n",
    "data[[\"Tweet_Token\", model_col_name]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create synthetic data with Back Translation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import itertools\n",
    "\n",
    "tgt_languages = [\"fr\", \"de\", \"es\"]\n",
    "language_combinations = list(itertools.combinations(tgt_languages, 2))\n",
    "\n",
    "\n",
    "def back_translate(text, forw_tokenizer, forw_model, backw_tokenizer, backw_model):\n",
    "    forward_input = forw_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    forward_output = forw_model.generate(forward_input)\n",
    "    forward_translation = forw_tokenizer.decode(forward_output[0], skip_special_tokens=True)\n",
    "\n",
    "    backward_input = backw_tokenizer.encode(forward_translation, return_tensors=\"pt\")\n",
    "    backward_output = backw_model.generate(backward_input)\n",
    "    backward_translation = backw_tokenizer.decode(backward_output[0], skip_special_tokens=True)\n",
    "\n",
    "    return backward_translation\n",
    "\n",
    "def multiple_back_translate(text, first_forw_tokenizer, first_forw_model, second_forw_tokenizer, second_forw_model, second_backw_tokenizer, second_backw_model, first_backw_tokenizer, first_backw_model):\n",
    "    first_forward_input = first_forw_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    first_forward_output = first_forw_model.generate(first_forward_input)\n",
    "    first_forward_translation = first_forw_tokenizer.decode(first_forward_output[0], skip_special_tokens=True)\n",
    "\n",
    "    second_forward_input = second_forw_tokenizer.encode(first_forward_translation, return_tensors=\"pt\")\n",
    "    second_forward_output = second_forw_model.generate(second_forward_input)\n",
    "    second_forward_translation = second_forw_tokenizer.decode(second_forward_output[0], skip_special_tokens=True)\n",
    "\n",
    "    second_backward_input = second_backw_tokenizer.encode(second_forward_translation, return_tensors=\"pt\")\n",
    "    second_backward_output = second_backw_model.generate(second_backward_input)\n",
    "    second_backward_translation = second_backw_tokenizer.decode(second_backward_output[0], skip_special_tokens=True)\n",
    "\n",
    "    first_backward_input = first_backw_tokenizer.encode(second_backward_translation, return_tensors=\"pt\")\n",
    "    first_backward_output = first_backw_model.generate(first_backward_input)\n",
    "    first_backward_translation = first_backw_tokenizer.decode(first_backward_output[0], skip_special_tokens=True)\n",
    "\n",
    "    return first_backward_translation\n",
    "\n",
    "forward_models, forward_tokenizers = {}, {}\n",
    "backward_models, backward_tokenizers = {}, {}\n",
    "\n",
    "for tgt_lang in tgt_languages:\n",
    "    forward_model_name = f'Helsinki-NLP/opus-mt-en-{tgt_lang}'\n",
    "    backward_model_name = f'Helsinki-NLP/opus-mt-{tgt_lang}-en'\n",
    "    forward_tokenizers[(\"en\",tgt_lang)] = MarianTokenizer.from_pretrained(forward_model_name)\n",
    "    forward_models[(\"en\",tgt_lang)] = MarianMTModel.from_pretrained(forward_model_name)\n",
    "    backward_tokenizers[(tgt_lang, \"en\")] = MarianTokenizer.from_pretrained(backward_model_name)\n",
    "    backward_models[(tgt_lang, \"en\")] = MarianMTModel.from_pretrained(backward_model_name)\n",
    "\n",
    "for tgt_lang_1, tgt_lang_2 in language_combinations:\n",
    "    forward_model_name = f'Helsinki-NLP/opus-mt-{tgt_lang_1}-{tgt_lang_2}'\n",
    "    backward_model_name = f'Helsinki-NLP/opus-mt-{tgt_lang_2}-{tgt_lang_1}'\n",
    "    forward_tokenizers[(tgt_lang_1, tgt_lang_2)] = MarianTokenizer.from_pretrained(forward_model_name)\n",
    "    forward_models[(tgt_lang_1, tgt_lang_2)] = MarianMTModel.from_pretrained(forward_model_name)\n",
    "    backward_tokenizers[(tgt_lang_2, tgt_lang_1)] = MarianTokenizer.from_pretrained(backward_model_name)\n",
    "    backward_models[(tgt_lang_2, tgt_lang_1)] = MarianMTModel.from_pretrained(backward_model_name)\n",
    "\n",
    "\n",
    "translation_cache = {}\n",
    "\n",
    "def translate_tweet(tweet, tgt_lang, second_tgt_lang=None):\n",
    "    cache_key = (tweet, tgt_lang, second_tgt_lang) if second_tgt_lang else (tweet, tgt_lang)\n",
    "\n",
    "    if cache_key in translation_cache:\n",
    "        return translation_cache[cache_key]\n",
    "    else:\n",
    "        if second_tgt_lang:\n",
    "            translation = multiple_back_translate(\n",
    "                tweet,\n",
    "                forward_tokenizers[(\"en\", tgt_lang)],\n",
    "                forward_models[(\"en\", tgt_lang)],\n",
    "                forward_tokenizers[(tgt_lang, second_tgt_lang)],\n",
    "                forward_models[(tgt_lang, second_tgt_lang)],\n",
    "                backward_tokenizers[(second_tgt_lang, tgt_lang)],\n",
    "                backward_models[(second_tgt_lang, tgt_lang)],\n",
    "                backward_tokenizers[(tgt_lang, \"en\")],\n",
    "                backward_models[(tgt_lang, \"en\")],\n",
    "            )\n",
    "        else:\n",
    "            translation = back_translate(\n",
    "                tweet,\n",
    "                forward_tokenizers[(\"en\", tgt_lang)],\n",
    "                forward_models[(\"en\", tgt_lang)],\n",
    "                backward_tokenizers[(tgt_lang, \"en\")],\n",
    "                backward_models[(tgt_lang, \"en\")],\n",
    "            )\n",
    "        translation_cache[cache_key] = translation\n",
    "        return translation\n",
    "\n",
    "for tgt_lang in tgt_languages:\n",
    "    data[f\"{tgt_lang} Back_Translate_Synthetic\"] = data[\"Tweet\"].apply(lambda tweet: translate_tweet(tweet, tgt_lang))\n",
    "\n",
    "for tgt_lang_1, tgt_lang_2 in language_combinations:\n",
    "   data[f\"{tgt_lang_1} - {tgt_lang_2} Back_Translate_Synthetic\"] = data[\"Tweet\"].apply(lambda tweet: translate_tweet(tweet, tgt_lang_1, tgt_lang_2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T14:53:34.499677Z",
     "end_time": "2023-04-19T14:56:46.373764Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate similarity scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def semantic_similarity(original_tweet, synthetic_tweet):\n",
    "    original_embedding = embed([original_tweet])\n",
    "    synthetic_embedding = embed([synthetic_tweet])\n",
    "    return np.inner(original_embedding, synthetic_embedding)[0][0]\n",
    "\n",
    "def compute_scores(data, col1, col2, prefix):\n",
    "    data[f\"{prefix}_Sim_Score\"] = data[[col1, col2]].apply(lambda x: semantic_similarity(x[col1], x[col2]), axis=1)\n",
    "    data[f\"{prefix}_Levenshtein_Score\"] = data[[col1, col2]].apply(lambda x: Levenshtein.distance(x[col1], x[col2]), axis=1)\n",
    "\n",
    "compute_scores(data, \"Tweet\", synonym_col_name, \"Synonyms\")\n",
    "compute_scores(data, \"Tweet\", fasttext_col_name, \"fasttext\")\n",
    "compute_scores(data, \"Tweet\", word2vec_col_name, \"word2vec\")\n",
    "compute_scores(data, \"Tweet\", model_col_name, \"GPT2\")\n",
    "\n",
    "for lang in tgt_languages:\n",
    "    compute_scores(data, \"Tweet\", f\"{lang} Back_Translate_Synthetic\", f\"{lang} Back_Translate\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T17:29:45.466650Z",
     "end_time": "2023-04-18T17:30:36.175099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file = f\"synth-output-{INPUT_FILE}-syn-{SYNONYM_PERCENTAGE}-emb-{EMBEDDING_PERCENTAGE}-gen-{SEED_PERCENTAGE}-lang-{tgt_languages}.csv\"\n",
    "\n",
    "data.to_csv(f\"../output/{output_file}\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
