{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/environment/notebooks/synthetic-tweets-in-sentiment-analysis/\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from libs.SyntheticDataGenerator import fill_missing_labels, fill_synthetic_data_percentage\n",
    "from libs.SentimentClassifier import preprocess, evaluation\n",
    "import torch\n",
    "import torch_directml\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else torch_directml.device())\n",
    "SYNTH_METHOD = \"synonyms\"\n",
    "TGT_LANGUAGES = [\"ru\"]\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 2\n",
    "\n",
    "data =pd.read_csv(\n",
    "    filepath_or_buffer='../data/data.csv',\n",
    "    sep='\\t',\n",
    "    encoding='utf8',\n",
    "    names=[\"ID\", \"Label\", \"Tweet\"]\n",
    "    )\n",
    "# gpt3_data = pd.read_csv(\n",
    "#     filepath_or_buffer='../data/ChatGPT.csv',\n",
    "#     sep='\\t',\n",
    "#     encoding='utf8',\n",
    "#     names=[\"Tweet\", \"Label\"]\n",
    "#     )\n",
    "data = data.sample(100)\n",
    "negative_data = data[data[\"Label\"] == \"negative\"]\n",
    "neutral_data = data[data[\"Label\"] == \"neutral\"]\n",
    "positive_data = data[data[\"Label\"] == \"positive\"]\n",
    "filled_data = data.sample(100)\n",
    "    #pd.concat([negative_data.sample(1000), neutral_data.sample(1000), positive_data.sample(1000)])\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(data, test_size=0.20, random_state=42)\n",
    "print(\"Labelverteilung ohne Datenaugmentation: \")\n",
    "print(train_dataset['Label'].value_counts(normalize=True))\n",
    "#filled_data, synth_word_ratio = fill_synthetic_data_percentage(train_dataset,\n",
    "#                                  percentage=2,\n",
    "#                                  method=SYNTH_METHOD,\n",
    "#                                 word_embedding_model=glove_model,\n",
    "#                                  tgt_languages=TGT_LANGUAGES,\n",
    "#                                  coverage_percentage=1)\n",
    "print(\"Labelverteilung mit Datenaugmentation: \")\n",
    "print(filled_data['Label'].value_counts(normalize=True))\n",
    "#print(f\"Synth word ratio: {synth_word_ratio:.2f}\")\n",
    "train_dataloader = preprocess(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = preprocess(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "evaluation(train_dataloader, val_dataloader, epochs=EPOCHS, device=device)\n",
    "filled_data_without_duplicates = filled_data.drop_duplicates(subset='Tweet', keep=\"first\")\n",
    "print(f\"{filled_data.size - filled_data_without_duplicates.size} entries are duplicates\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data =pd.read_csv(\n",
    "    filepath_or_buffer='../data/data.csv',\n",
    "    sep='\\t',\n",
    "    encoding='utf8',\n",
    "    names=[\"ID\", \"Label\", \"Tweet\"]\n",
    "    )\n",
    "\n",
    "data['Word Count'] = data['Tweet'].str.split().apply(len)\n",
    "mean_length_by_label = data.groupby('Label')['Word Count'].mean()\n",
    "print(mean_length_by_label)\n",
    "sns.histplot(mean_length_by_label, x=\"Label\", y=\"Word Count\")\n",
    "plt.title('Histogram of Tweet Lengths')\n",
    "plt.xlabel('Length of Tweet')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "negative_data = data[data[\"Label\"] == \"negative\"]\n",
    "neutral_data = data[data[\"Label\"] == \"neutral\"]\n",
    "positive_data = data[data[\"Label\"] == \"positive\"]\n",
    "print(negative_data.size)\n",
    "text = ' '.join(neutral_data['Tweet'])\n",
    "# Tokenize the text into individual words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords from the tokens\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords_list and word not in string.punctuation]\n",
    "\n",
    "# Calculate the frequency distribution of words\n",
    "freq_dist = FreqDist(filtered_tokens)\n",
    "\n",
    "# Get the top 15 most frequent words\n",
    "top_15_words = freq_dist.most_common(50)\n",
    "\n",
    "# Print the top 15 words\n",
    "for word, frequency in top_15_words:\n",
    "    print(word, frequency)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data =pd.read_csv(\n",
    "    filepath_or_buffer='../data/Sentiment140.csv',\n",
    "    encoding='latin-1',\n",
    "    names=[\"Label\", \"ID\", \"Timestamp\", \"Query\", \"Username\", \"Tweet\"]\n",
    "    )\n",
    "data = data.sample(frac=1)\n",
    "data = data.sample(50000)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = data['Label'].value_counts()\n",
    "print(label_counts)\n",
    "# Set the desired count for each label\n",
    "desired_count = min(label_counts)\n",
    "# Filter the DataFrame to have the same count of labels\n",
    "filtered_df = data.groupby('Label').apply(lambda x: x.sample(desired_count))\n",
    "\n",
    "# Reset the index of the filtered DataFrame\n",
    "#filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "palette ={\"neutral\": \"orange\",\"positive\": \"red\",\"negative\": \"blue\",}\n",
    "# Annahme: Sie haben einen DataFrame namens \"train_data\" mit einer Spalte \"Label\", die die Labels enthält\n",
    "# und die Werte der Labels sind in Form von Zeichenketten (Strings) gegeben.\n",
    "\n",
    "# Verwenden Sie die \"value_counts()\" Funktion, um die Anzahl der Vorkommen jedes Labels zu ermitteln\n",
    "labels = data['Label']\n",
    "label_counts = labels.value_counts()\n",
    "\n",
    "# Erstellen Sie ein Kuchendiagramm der Label-Verteilung mit Seaborn\n",
    "plt.figure(figsize=(8, 6))  # Festlegen der Diagrammgröße\n",
    "textprops={'color':\"black\", 'size': 15}\n",
    "# Erstellen des Kuchendiagramms\n",
    "pie_chart = plt.pie(label_counts, labels=label_counts.index, \n",
    "        autopct='%1.1f%%', \n",
    "        colors=palette.values(), textprops=textprops)\n",
    "pie_chart[0][0].get_figure().savefig(\"../figures/word_distribution.svg\")\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def label_to_number(label):\n",
    "    label_mapping = {\n",
    "        'negative' : 0,\n",
    "        'neutral' : 1,\n",
    "        'positive' : 2\n",
    "    }\n",
    "\n",
    "    return label_mapping[label]\n",
    "\n",
    "\n",
    "labels = labels.apply(lambda x : label_to_number(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "\n",
    "# Tokenize and pad the tweets\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "\n",
    "def tweet_pipeline(tweet):\n",
    "    encoded_dict = tokenizer.encode_plus(tweet,\n",
    "                                         add_special_tokens = True,\n",
    "                                         padding= 'max_length',\n",
    "                                         return_attention_mask = True,\n",
    "                                         return_tensors = 'pt')\n",
    "    input_ids.append(encoded_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "tweets = tweets.apply(lambda tweet : tweet_pipeline(tweet))\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.SentimentClassifier import k_cross_fold_validation, evaluation\n",
    "from transformers import logging\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "k = 2\n",
    "batch_size = 5\n",
    "epochs = 2\n",
    "device = \"cpu\"\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.4 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#k_cross_fold_validation(dataset,k,epochs,batch_size,device)\n",
    "evaluation(train_dataloader,validation_dataloader,epochs,device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: 'neutral',\n",
    "    1: 'positive',\n",
    "    2: 'negative'\n",
    "}\n",
    "\n",
    "all_cm = [i['Valid. Confusion Matrix'] for i in training_stats]\n",
    "\n",
    "avrg_confusion_matrix = torch.ceil(torch.mean(torch.stack(all_cm) , dim=0))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "class_names = list(class_mapping.values())\n",
    "df_cm = pd.DataFrame(avrg_confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
