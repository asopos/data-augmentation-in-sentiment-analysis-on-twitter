{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from libs.SyntheticDataGenerator import fill_missing_labels, fill_synthetic_data_percentage\n",
    "from libs.SentimentClassifier import preprocess, evaluation\n",
    "\n",
    "import torch_directml\n",
    "\n",
    "device = torch_directml.device()\n",
    "\n",
    "data =pd.read_csv(\n",
    "    filepath_or_buffer='../data/data.csv',\n",
    "    sep='\\t',\n",
    "    encoding='utf8',\n",
    "    names=[\"ID\", \"Label\", \"Tweet\"]\n",
    "    )\n",
    "\n",
    "data = data.sample(100)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataloader = preprocess(train_dataset, batch_size=10)\n",
    "val_dataset = preprocess(val_dataset, batch_size=10)\n",
    "\n",
    "evaluation(train_dataloader, val_dataset, epochs=2, device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data =pd.read_csv(\n",
    "    filepath_or_buffer='../data/training.1600000.processed.noemoticon.csv',\n",
    "    encoding='latin-1',\n",
    "    names=[\"Label\", \"ID\", \"Timestamp\", \"Query\", \"Username\", \"Tweet\"]\n",
    "    )\n",
    "data = data.sample(frac=1)\n",
    "data = data.sample(500000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = data['Label'].value_counts()\n",
    "print(label_counts)\n",
    "# Set the desired count for each label\n",
    "desired_count = min(label_counts)\n",
    "# Filter the DataFrame to have the same count of labels\n",
    "filtered_df = data.groupby('Label').apply(lambda x: x.sample(desired_count))\n",
    "\n",
    "# Reset the index of the filtered DataFrame\n",
    "#filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Annahme: Sie haben einen DataFrame namens \"train_data\" mit einer Spalte \"Label\", die die Labels enthält\n",
    "# und die Werte der Labels sind in Form von Zeichenketten (Strings) gegeben.\n",
    "\n",
    "# Verwenden Sie die \"value_counts()\" Funktion, um die Anzahl der Vorkommen jedes Labels zu ermitteln\n",
    "labels = data['Label']\n",
    "label_counts = labels.value_counts()\n",
    "\n",
    "# Erstellen Sie ein Kuchendiagramm der Label-Verteilung mit Seaborn\n",
    "plt.figure(figsize=(8, 6))  # Festlegen der Diagrammgröße\n",
    "sns.set(style=\"darkgrid\")  # Festlegen des Stils\n",
    "\n",
    "# Erstellen des Kuchendiagramms\n",
    "plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "plt.title('Verteilung der Labels')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def label_to_number(label):\n",
    "    label_mapping = {\n",
    "        'negative' : 0,\n",
    "        'neutral' : 1,\n",
    "        'positive' : 2\n",
    "    }\n",
    "\n",
    "    return label_mapping[label]\n",
    "\n",
    "\n",
    "labels = labels.apply(lambda x : label_to_number(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "\n",
    "# Tokenize and pad the tweets\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "\n",
    "def tweet_pipeline(tweet):\n",
    "    encoded_dict = tokenizer.encode_plus(tweet,\n",
    "                                         add_special_tokens = True,\n",
    "                                         padding= 'max_length',\n",
    "                                         return_attention_mask = True,\n",
    "                                         return_tensors = 'pt')\n",
    "    input_ids.append(encoded_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "tweets = tweets.apply(lambda tweet : tweet_pipeline(tweet))\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.SentimentClassifier import k_cross_fold_validation, evaluation\n",
    "from transformers import logging\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "k = 2\n",
    "batch_size = 5\n",
    "epochs = 2\n",
    "device = \"cpu\"\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.4 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#k_cross_fold_validation(dataset,k,epochs,batch_size,device)\n",
    "evaluation(train_dataloader,validation_dataloader,epochs,device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: 'neutral',\n",
    "    1: 'positive',\n",
    "    2: 'negative'\n",
    "}\n",
    "\n",
    "all_cm = [i['Valid. Confusion Matrix'] for i in training_stats]\n",
    "\n",
    "avrg_confusion_matrix = torch.ceil(torch.mean(torch.stack(all_cm) , dim=0))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "class_names = list(class_mapping.values())\n",
    "df_cm = pd.DataFrame(avrg_confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
